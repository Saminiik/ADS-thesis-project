---
title: "Elastic Net"
author: |
  **Author:**  
  
  | Name              | Student Number | Email Address                           |
  |:------------------|:--------------|:----------------------------------------|
  | Marion Späth     | 2772981        | m.m.spath@uu.nl                        |
date: "`r Sys.Date()`"
output:
    html_document:
      df_print: paged
      toc: true
      toc_float: yes
      number_sections: false
      highlight: haddock
---

# Intro

This Markdown documents the Elastic Net modelling process. In short, data is imported, numeric variables are standardised, categorical variables are factorised. An initial Random Effects (RE) Model shows that only very little variation can be explained by individuals-level, indicating that the clustered nature of the data likely wont distort the results much if not accounted for. Elastic Net models are conducted using 10-fold cross-validation and are run both on the logged (difference) outcome variable and on the residuals stemming from the RE model. The idea of the latter is that the residuals have been "cleared" of the nested data structure. RMSE and R-Squared of the CV-solutions and of the test data predictions are compared across models.

# Load Data, Libraries, Custom Functions

```{r echo=T, warning=FALSE, message=F}
set.seed(123)  # set seed for reproducibility

# Required packages
required_packages <- c("dplyr", "tidyverse", "rio", "gridExtra", "ggplot2", "lme4", "caret", "glmnet")

# Install any missing packages, then load all
for (pkg in required_packages) { if (!requireNamespace(pkg, quietly = TRUE)) { install.packages(pkg)}
  library(pkg, character.only = TRUE)}

cattle <- import("C:/Users/Marion Späth/Desktop/ADS/Thesis/Data/cattle_df.xlsx")
goat <- import("C:/Users/Marion Späth/Desktop/ADS/Thesis/Data/goat_df.xlsx")

```


To extract the best result (or tuning parameters) of the Elastic Net.
```{r}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
```

# Prepare variables

Standardise numeric variables, factorise categorical variables.

```{r}
cattle_s <- cattle %>% select(-c( ...1, purchase_bin, advise_vip))
goat_s <- goat %>% select(-c( ...1, purchase_bin, advise_vip))

# cs_cs_diff_post_cattle, cs_cs_ratio_post_cattle, id, wave
#numeric_columns = ['buy_nr_goat', 'n_previd_goat', 'ratio_insured_goat', 'age_constant', 'number_adults', 'number_minors']

cat_vars <- c('afm_language', 'agric_land', 'amh_language', 'educ_recoded_constant', 'eng_language', 'expend', 'irrigated_land_bin', 'educ_child_recoded', 'activity_child_recoded', 'household_description',  'main_info_source_recoded', 'religion_recoded', 'owns_phone', 'household_moved', 'why_not_purchase_recoded', 'know_vip', 'trust_vip')

num_vars <- c('buy_nr_cattle', 'n_previd_cattle', 'ratio_insured_cattle', 'age_constant', 'number_adults', 'number_minors')


cattle_s[cat_vars] <- lapply(cattle_s[cat_vars], as.factor)
goat_s[cat_vars] <- lapply(goat_s[cat_vars], as.factor)

cattle_s$buy_cattle <- as.factor(cattle_s$buy_cattle)
goat_s$buy_goat <- as.factor(goat_s$buy_goat)

cattle_s$buy_nr_cattle_z <-        as.numeric(scale(cattle_s$buy_nr_cattle))
cattle_s$n_previd_cattle_z <-      as.numeric(scale(cattle_s$n_previd_cattle))
cattle_s$ratio_insured_cattle_z <- as.numeric(scale(cattle_s$ratio_insured_cattle))
cattle_s$age_constant_z <-  as.numeric(scale(cattle_s$age_constant))
cattle_s$number_adults_z <- as.numeric(scale(cattle_s$number_adults))
cattle_s$number_minors_z <- as.numeric(scale(cattle_s$number_minors))

goat_s$buy_nr_goat_z <-        as.numeric(scale(goat_s$buy_nr_goat))
goat_s$n_previd_goat_z <-      as.numeric(scale(goat_s$n_previd_goat))
goat_s$ratio_insured_goat_z <- as.numeric(scale(goat_s$ratio_insured_goat))
goat_s$age_constant_z <-  as.numeric(scale(goat_s$age_constant))
goat_s$number_adults_z <- as.numeric(scale(goat_s$number_adults))
goat_s$number_minors_z <- as.numeric(scale(goat_s$number_minors))

# take log for normal distribution of variables
cattle_s$cs_diff_log <- log(abs(cattle_s$cs_cs_diff_post_cattle)+1)
goat_s$cs_diff_log <- log(abs(goat_s$cs_cs_diff_post_goat)+1)

#cattle_s[num_vars] <- lapply(cattle_s[num_vars], function(x) as.numeric(scale(x)))
```

# Run RE model

The random intercept model, which includes only a random effect for individual ID to account for the panel structure of the data, shows that only 4.78% of the variance in the outcome can be attributed to differences between individuals. Thus, there is only low level of dependency. The scatterplot shows that the original outcome variable and the residuals of the RE model are almost perfectly linear. Thus, using either the extracted residuals (which have been "cleared" from the panel data structure) or the original outcome variable for subsequent models should not lead to very different results.

CATTLE:
```{r}
# Fit random intercept model
mixed_model <- lmer(cs_diff_log ~ 1 + (1 | id), data = cattle_s)

# Extract residuals (variation not explained by person-level intercepts)
cattle_s$resid_mixed <- resid(mixed_model)

ggplot(data=cattle_s, aes(x=cs_diff_log, y=resid_mixed)) +
  geom_point()

summary(mixed_model)
```

### Intraclass Correlation Coefficient (ICC) Cattle

```{r}
# compute icc
icc <- 0.1416 / (0.1416 + 2.8230) *100
icc
```
Only 4.78% of the variance between individuals in the outcome can be explained. This is relatively low, indicating that the individual-level clustering likely doesn't distort the results much when not accounted for.

```{r}
# Fit random intercept model
mixed_model2 <- lmer(cs_diff_log ~ 1 + (1 | id), data = goat_s)

# Extract residuals (variation not explained by person-level intercepts)
goat_s$resid_mixed <- resid(mixed_model2)

ggplot(data=goat_s, aes(x=cs_diff_log, y=resid_mixed)) +
  geom_point()

summary(mixed_model2)
```
### Intraclass Correlation Coefficient (ICC) Cattle

```{r}
# compute icc
icc <- 0.1459 / (0.1416 + 2.0435) *100
icc
```
Only 6.68% of the variance between individuals in the outcome can be explained. Again, this is relatively low, indicating that the individual-level clustering likely doesn't distort the results much when not accounted for.


# Elastic Net

## Cattle 
Adjust variables and split into train and test set. Prepare for cross-validation.
I do this once using the original (logged) outcome and once using the residuals from the RE model to compare results.
```{r}
# original outcome
model_vars <- cattle_s %>% select(c( "afm_language", "age_constant_z", "agric_land", "amh_language", "educ_recoded_constant", 
                                      "eng_language", "expend", "irrigated_land_bin", "buy_cattle", "buy_nr_cattle_z", 
                                      "n_previd_cattle_z", "number_minors_z", "educ_child_recoded", "activity_child_recoded", 
                                      "household_description", "number_adults_z", "main_info_source_recoded", 
                                      "religion_recoded", "owns_phone", "household_moved", "why_not_purchase_recoded",
                                      "know_vip",  "trust_vip", "ratio_insured_cattle_z", "cs_diff_log"))
# residual outcome
model_vars2 <- cattle_s %>% select(c( "afm_language", "age_constant_z", "agric_land", "amh_language", "educ_recoded_constant", 
                                      "eng_language", "expend", "irrigated_land_bin", "buy_cattle", "buy_nr_cattle_z", 
                                      "n_previd_cattle_z", "number_minors_z", "educ_child_recoded", "activity_child_recoded", 
                                      "household_description", "number_adults_z", "main_info_source_recoded", 
                                      "religion_recoded", "owns_phone", "household_moved", "why_not_purchase_recoded",
                                      "know_vip",  "trust_vip", "ratio_insured_cattle_z", "resid_mixed"))

# original outcome
split_idx1 <- createDataPartition(model_vars$cs_diff_log, p = 0.8, list = FALSE)
train_data1 <- model_vars[split_idx1, ]
test_data1 <- model_vars[-split_idx1, ]

# residual outcome
split_idx2 <- createDataPartition(model_vars2$resid_mixed, p = 0.8, list = FALSE)
train_data2 <- model_vars2[split_idx2, ]
test_data2 <- model_vars2[-split_idx2, ]

# cross-validation
cv_10 = trainControl(method = "cv", number = 10)
```

Elastic net with original (logged) outcome and 10-fold CV.
```{r}
enet1 = train(cs_diff_log ~  . , 
              data = model_vars,
              method = "glmnet",
              trControl = cv_10,
              tuneLength = 10)
best_res_enet1 <- get_best_result(enet1)
best_res_enet1

# enet1$bestTune
```

Elastic net with residual outcome and 10-fold CV.

```{r}
enet2 = train(resid_mixed ~  .,
              data = model_vars2,
              method = "glmnet",
              trControl = cv_10,
              tuneLength = 10)
best_res_enet2 <- get_best_result(enet2)
best_res_enet2
```

The following code is not evaluated in the Markdown due to its long runtime. My goal was to investigate the change in performance measures when adopting the leave one out cross-validation approach as this increases the training data which could be helpful with smaller datasets. I copied the result below the code. While the choice of the tuning parameters alpha and lambda changes, the RMSE, MAE and RSquared are all very slightly worse. Thus, just based on the cv values of the training data using 10-fold CV is the better choice in terms of performance and runtime. 

```{r eval=FALSE}
loocv_ctrl <- trainControl(method = "LOOCV")
enet3 = train(resid_mixed ~  .,
              data = model_vars2,
              method = "glmnet",
              trControl = loocv_ctrl,
              tuneLength = 10)
best_res_enet3 <- get_best_result(enet3)
best_res_enet3
' alpha  lambda     RMSE      Rsquared     MAE 
  0.7	  0.0112199 	1.360208	0.3133747	  1.03679'
```

## Make and Prepare Predictions

```{r}
# Predict and assess performance
test_preds1 <- predict(enet1, newdata = test_data1)
cat("Performance on test set using the original logged outcome variable: \n")
print(postResample(pred = test_preds1, obs = test_data1$cs_diff_log))

test_preds2 <- predict(enet2, newdata = test_data2)
cat("Performance on test set using the RESIDUAL outcome variable and 10-fold CV: \n")
print(postResample(pred = test_preds2, obs = test_data2$resid_mixed))


#test_preds3 <- predict(enet3, newdata = test_data2)
#cat("Performance on test set using the RESIDUAL outcome variable and LOOCV: \n")
#print(postResample(pred = test_preds3, obs = test_data2$resid_mixed))

'Performance on test set using the RESIDUAL outcome variable and LOOCV: 
     RMSE    Rsquared       MAE 
1.3793528    0.3192034   1.0468857 '
```
The RMSE is largely similar across model specifications, but slightly better when using the residual variable than when using the original (logged) variable (-0.08). The R-Squared is lower for the model with the residual variable because part of the variation has already been explained by the RE model. Considering this, the R-Squared is essentially the same. Thus, both outcome variables are defendable choices. 

## Goat 

Adjust variables and split into train and test set. Prepare for cross-validation.
I do this once using the original (logged) outcome and once using the residuals from the RE model to compare results.
```{r}
# original outcome
model_vars3 <- goat_s %>% select(c( "afm_language", "age_constant_z", "agric_land", "amh_language", "educ_recoded_constant", 
                                      "eng_language", "expend", "irrigated_land_bin", "buy_goat", "buy_nr_goat_z", 
                                      "n_previd_goat_z", "number_minors_z", "educ_child_recoded", "activity_child_recoded", 
                                      "household_description", "number_adults_z", "main_info_source_recoded", 
                                      "religion_recoded", "owns_phone", "household_moved", "why_not_purchase_recoded",
                                      "know_vip",  "trust_vip", "ratio_insured_goat_z", "cs_diff_log"))
# residual outcome
model_vars4 <- goat_s %>% select(c( "afm_language", "age_constant_z", "agric_land", "amh_language", "educ_recoded_constant", 
                                      "eng_language", "expend", "irrigated_land_bin", "buy_goat", "buy_nr_goat_z", 
                                      "n_previd_goat_z", "number_minors_z", "educ_child_recoded", "activity_child_recoded", 
                                      "household_description", "number_adults_z", "main_info_source_recoded", 
                                      "religion_recoded", "owns_phone", "household_moved", "why_not_purchase_recoded",
                                      "know_vip",  "trust_vip", "ratio_insured_goat_z", "resid_mixed"))

# original outcome
split_idx3 <- createDataPartition(model_vars3$cs_diff_log, p = 0.8, list = FALSE)
train_data3 <- model_vars3[split_idx3, ]
test_data3 <- model_vars3[-split_idx3, ]

# residual outcome
split_idx4 <- createDataPartition(model_vars4$resid_mixed, p = 0.8, list = FALSE)
train_data4 <- model_vars4[split_idx4, ]
test_data4 <- model_vars4[-split_idx4, ]

# cross-validation
cv_10 = trainControl(method = "cv", number = 10)
```

Elastic net with original (logged) outcome and 10-fold CV.
```{r}
enet3 = train(cs_diff_log ~  . , # ^2
              data = model_vars3,
              method = "glmnet",
              trControl = cv_10,
              tuneLength = 10)
best_res_enet3 <- get_best_result(enet3)
best_res_enet3

# enet1$bestTune
```

Elastic net with residual outcome and 10-fold CV.

```{r}
enet4 = train(resid_mixed ~  .,
              data = model_vars4,
              method = "glmnet",
              trControl = cv_10,
              tuneLength = 10)
best_res_enet4 <- get_best_result(enet4)
best_res_enet4
```

## Make and Prepare Predictions

```{r}
# Predict and assess performance - GOAT
test_preds3 <- predict(enet3, newdata = test_data3)
cat("Performance on test set using the original logged outcome variable: \n")
print(postResample(pred = test_preds3, obs = test_data3$cs_diff_log))

test_preds4 <- predict(enet4, newdata = test_data4)
cat("Performance on test set using the RESIDUAL outcome variable and 10-fold CV: \n")
print(postResample(pred = test_preds4, obs = test_data4$resid_mixed))
```



# Variable / Parameter inspection

## Cattle

Here, I use enet2, the model with the residual outcome and 10-fold CV. (Cattle)

For simple variable overview:
```{r}
# Extract coefficients at best lambda as a matrix
elnet_coefs <- as.matrix(coef(enet2$finalModel, s = enet2$bestTune$lambda))

# Identify variables with zero coefficients (excluding the intercept)
eliminated_vars <- rownames(elnet_coefs)[elnet_coefs[, 1] == 0 & rownames(elnet_coefs) != "(Intercept)"]

noteliminated_vars <- rownames(elnet_coefs)[elnet_coefs[, 1] !=0 & rownames(elnet_coefs) != "(Intercept)"]

cat("Eliminated variables: \n", eliminated_vars, "\n")

cat("Non-zero variables: \n", noteliminated_vars)
```
Get and visualise coefficients:
```{r}
# Extract and convert coefficients to a matrix
elnet_coefs <- as.matrix(coef(enet2$finalModel, s = enet2$bestTune$lambda))

# Turn into a data frame
coef_df <- data.frame(variable = rownames(elnet_coefs),
                      coefficient = elnet_coefs[, 1],
                      row.names = NULL)

# Exclude intercept (optional)
coef_df <- coef_df[coef_df$variable != "(Intercept)", ]

# Add indicator for whether the variable was eliminated
coef_df$eliminated <- coef_df$coefficient == 0

# Sort by absolute value of coefficients, descending
coef_df <- coef_df[order(abs(coef_df$coefficient), decreasing = TRUE), ]
```


<div style="overflow: auto; width: 1600px; height: 500px;">
```{r, echo=FALSE, fig.height=8, fig.width=12}
#library(plotly)

ggplot(data = coef_df, aes(x = reorder(variable, coefficient), y = coefficient)) +
  geom_col() +
  coord_flip() +
  labs(x = "Variable", y = "Coefficient", title = "Elastic Net Coefficients") +
  scale_y_continuous(limits = c(-1.25, 0.5), breaks = seq(-1.5, 0.5, by = 0.5)) +
  theme_minimal()

#ggplotly(p)

p <- ggplot(data = coef_df, aes(x = reorder(variable, coefficient), y = coefficient)) +
  geom_col() +
  coord_flip() +
  labs(x = "Variable", y = "Coefficient", title = "Elastic Net Coefficients") +
  scale_y_continuous(limits = c(-1.25, 0.5), breaks = seq(-1.5, 0.5, by = 0.5)) +
  theme_minimal()

ggsave("Plot/plot_cattle.png", plot = p, width = 12, height = 8, dpi = 300)

```
</div>

Only relatively few variables (5 out of 39) were eliminated (/shrunken to zero).


## Goat

Here, I use enet4, the model with the residual outcome and 10-fold CV. (Cattle)

For simple variable overview:
```{r}
# Extract coefficients at best lambda as a matrix
elnet_coefs_goat <- as.matrix(coef(enet4$finalModel, s = enet4$bestTune$lambda))

# Identify variables with zero coefficients (excluding the intercept)
eliminated_vars_goat <- rownames(elnet_coefs_goat)[elnet_coefs_goat[, 1] == 0 & rownames(elnet_coefs_goat) != "(Intercept)"]

noteliminated_vars_goat <- rownames(elnet_coefs_goat)[elnet_coefs_goat[, 1] !=0 & rownames(elnet_coefs_goat) != "(Intercept)"]

cat("Eliminated variables: \n", eliminated_vars_goat, "\n")

cat("Non-zero variables: \n", noteliminated_vars_goat)
```

Get and visualise coefficients:
```{r}
# Extract and convert coefficients to a matrix
elnet_coefs_goat <- as.matrix(coef(enet4$finalModel, s = enet4$bestTune$lambda))

# Turn into a data frame
coef_df_goat <- data.frame(variable = rownames(elnet_coefs_goat),
                      coefficient = elnet_coefs_goat[, 1],
                      row.names = NULL)

# Exclude intercept (optional)
coef_df_goat <- coef_df_goat[coef_df_goat$variable != "(Intercept)", ]

# Add indicator for whether the variable was eliminated
coef_df_goat$eliminated <- coef_df_goat$coefficient == 0

# Sort by absolute value of coefficients, descending
coef_df_goat <- coef_df_goat[order(abs(coef_df_goat$coefficient), decreasing = TRUE), ]
```

<div style="overflow: auto; width: 1600px; height: 500px;">
```{r, echo=FALSE, fig.height=8, fig.width=12}
#library(plotly)

ggplot(data = coef_df_goat, aes(x = reorder(variable, coefficient), y = coefficient)) +
  geom_col() +
  coord_flip() +
  labs(x = "Variable", y = "Coefficient", title = "Elastic Net Coefficients") +
  scale_y_continuous(limits = c(-1.25, 0.5), breaks = seq(-1.5, 0.5, by = 0.5)) +
  theme_minimal()

#ggplotly(p)

p_goat <- ggplot(data = coef_df_goat, aes(x = reorder(variable, coefficient), y = coefficient)) +
  geom_col() +
  coord_flip() +
  labs(x = "Variable", y = "Coefficient", title = "Elastic Net Coefficients") +
  scale_y_continuous(limits = c(-1.25, 0.5), breaks = seq(-1.5, 0.5, by = 0.5)) +
  theme_minimal()

ggsave("Plot/plot_goat.png", plot = p_goat, width = 12, height = 8, dpi = 300)

```
</div>

